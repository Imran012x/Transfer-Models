{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imran012x/Hilsha_CoLab/blob/main/1_CNN%2Btransfer_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Co-Lab -->> Drive"
      ],
      "metadata": {
        "id": "WrRU5QMl0UZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uaVTAQGY0a4A",
        "outputId": "2a7b2515-850c-4b71-eda5-a34d92bfe82b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# # Upload a file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Get the file name\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "# print(f\"Uploaded file: {file_name}\")\n"
      ],
      "metadata": {
        "id": "rk9AmgjLrSJu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "bIrUcpjq0P9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/data_fish_224_12k.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('')"
      ],
      "metadata": {
        "id": "JP345hvS1nk1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "keXKd25Inyvb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ilish_pic= os.listdir('/content/ilish/')\n",
        "print(ilish_pic[0:5])\n",
        "print(ilish_pic[-5:])\n",
        "chandana_pic= os.listdir('/content/chandana/')\n",
        "sardin_pic= os.listdir('/content/sardin/')\n",
        "sardinella_pic= os.listdir('/content/sardinella/')\n",
        "punctatus_pic= os.listdir('/content/punctatus/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMyf2SoU2C0T",
        "outputId": "791ea2ce-494d-4eef-84da-5bed22b62db9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['977.webp', '327.webp', '1246.webp', '2809.webp', '2231.webp']\n",
            "['1294.webp', '875.webp', '2973.webp', '1250.webp', '487.webp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ilish   images:', len(ilish_pic))\n",
        "print('chandana  images:', len(chandana_pic))\n",
        "print('sardin   images:', len(sardin_pic))\n",
        "print('sardinella  images:', len(sardinella_pic))\n",
        "print('punctatus images:', len(sardinella_pic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU6TyDxt2Hp2",
        "outputId": "496a96ed-a779-4f7b-f2f4-e9911bce5de4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ilish   images: 3000\n",
            "chandana  images: 2000\n",
            "sardin   images: 3000\n",
            "sardinella  images: 2000\n",
            "punctatus images: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ilish_pic_labels = [0]*len(ilish_pic)\n",
        "chandana_pic_labels = [1]*len(chandana_pic)\n",
        "sardin_pic_labels = [2]*len(sardin_pic)\n",
        "sardinella_pic_labels = [3]*len(sardinella_pic)\n",
        "punctatus_pic_labels = [4]*len(punctatus_pic)\n",
        "\n",
        "\n",
        "print(ilish_pic_labels[0:5])\n",
        "\n",
        "print(chandana_pic_labels[0:5])\n",
        "\n",
        "print(sardin_pic_labels[0:5])\n",
        "\n",
        "print(sardinella_pic_labels[0:5])\n",
        "\n",
        "print(punctatus_pic_labels[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50rePefD2Km6",
        "outputId": "52a8f6d3-5af1-4144-fca0-eb8c63d0fa70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1]\n",
            "[2, 2, 2, 2, 2]\n",
            "[3, 3, 3, 3, 3]\n",
            "[4, 4, 4, 4, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ilish_pic_labels + chandana_pic_labels + sardin_pic_labels + sardinella_pic_labels + punctatus_pic_labels\n",
        "\n",
        "print(len(labels))\n",
        "print(labels[0:5])\n",
        "print(labels[-5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwYTot_v1QTw",
        "outputId": "87d494f6-80f4-4f2f-8d59-b52461686461"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12000\n",
            "[0, 0, 0, 0, 0]\n",
            "[4, 4, 4, 4, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Parameters (set different numbers for each class)\n",
        "ilish_image_number = len(ilish_pic)\n",
        "chandana_image_number = len(chandana_pic)\n",
        "sardin_image_number = len(sardin_pic)\n",
        "sardinella_image_number = len(sardinella_pic)\n",
        "punctatus_image_number = len(punctatus_pic)\n",
        "\n",
        "\n",
        "# Initialize the list to store the images\n",
        "data = []\n",
        "\n",
        "def process_images(image_path, image_number):\n",
        "    image_labels = sorted(os.listdir(image_path))  # Ensure images are in order\n",
        "    random.shuffle(image_labels)  # Shuffle to get random ones\n",
        "    selected_images = []\n",
        "    for img_file in image_labels[:image_number]:\n",
        "        image = Image.open(os.path.join(image_path, img_file))\n",
        "        image = image.resize((224, 224))\n",
        "        image = image.convert('RGB')\n",
        "        image = np.array(image)\n",
        "        selected_images.append(image)\n",
        "    return selected_images\n",
        "\n",
        "# Process images for each category\n",
        "data.extend(process_images('/content/ilish/', ilish_image_number))\n",
        "data.extend(process_images('/content/chandana/', chandana_image_number))\n",
        "data.extend(process_images('/content/sardin/', sardin_image_number))\n",
        "data.extend(process_images('/content/sardinella/', sardinella_image_number))\n",
        "data.extend(process_images('/content/punctatus/', punctatus_image_number))"
      ],
      "metadata": {
        "id": "6gu2Bfj02ht1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to numpy array for easy processing\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Count the number of images in each class\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "# Print the number of images for each class\n",
        "class_counts = dict(zip(unique, counts))\n",
        "print(f\"Class counts: {class_counts}\")\n",
        "# converting image list and label list to numpy arrays\n",
        "\n",
        "X = np.array(data)\n",
        "Y = np.array(labels)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "FyNKmCny1UYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "print(X.shape, X_train.shape, X_test.shape)\n",
        "# scaling the data\n",
        "\n",
        "X_train_scaled = X_train/255\n",
        "\n",
        "X_test_scaled = X_test/255\n",
        "\n",
        "X_train[0]"
      ],
      "metadata": {
        "id": "a_Auo6ip2l_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optuna-Hyper parameter Training"
      ],
      "metadata": {
        "id": "C4tpEFOX5BBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install optuna\n",
        "# import optuna\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "# from tensorflow.keras.optimizers import RMSprop\n",
        "# from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, accuracy_score, f1_score\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Define model creation function\n",
        "# def create_model(learning_rate, num_filters, kernel_size):\n",
        "#     input_shape = (224, 224, 3)\n",
        "#     base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "#     x = base_model.output\n",
        "#     x = Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', padding='same')(x)\n",
        "#     x = MaxPooling2D((2, 2))(x)\n",
        "#     x = Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu', padding='same')(x)\n",
        "#     x = MaxPooling2D((2, 2))(x)\n",
        "#     x = GlobalAveragePooling2D()(x)\n",
        "#     x = Dense(256, activation='relu')(x)\n",
        "#     output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "#     model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "#     for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "\n",
        "#     model.compile(optimizer=RMSprop(learning_rate=learning_rate),\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Define Optuna objective function\n",
        "# def objective(trial):\n",
        "#     learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "#     num_filters = trial.suggest_int('num_filters', 16, 128)\n",
        "#     kernel_size = trial.suggest_categorical('kernel_size', [3, 7])\n",
        "\n",
        "#     model = create_model(learning_rate, num_filters, kernel_size)\n",
        "#     model.fit(X_train_scaled, Y_train, epochs=8, validation_data=(X_test_scaled, Y_test), verbose=0)\n",
        "#     score = model.evaluate(X_test_scaled, Y_test, verbose=0)\n",
        "#     return score[1]\n",
        "\n",
        "# # Run Optuna study\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(objective, n_trials=30)\n",
        "\n",
        "# # Get best parameters\n",
        "# best_params = study.best_params\n",
        "# print(\"Best hyperparameters: \", best_params)\n",
        "\n",
        "# # Train final model with best parameters\n",
        "# model = create_model(best_params['learning_rate'], best_params['num_filters'], best_params['kernel_size'])\n",
        "# history = model.fit(X_train_scaled, Y_train,\n",
        "#                     validation_split=0.2,\n",
        "#                     epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# # Evaluate model\n",
        "# val_loss, val_accuracy = model.evaluate(X_test_scaled, Y_test, verbose=0)\n",
        "# print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NDybI1TUDb7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unsupervised Learning"
      ],
      "metadata": {
        "id": "NnTD1EDHCy4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import zipfile\n",
        "# import os\n",
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "# from sklearn.cluster import KMeans\n",
        "# from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# # Set Random Seed for Reproducibility\n",
        "# np.random.seed(42)\n",
        "# tf.random.set_seed(42)\n",
        "\n",
        "# # ðŸ“Œ Extract the ZIP File\n",
        "# zip_file = \"/content/drive/MyDrive/mixed.zip\"  # Update this path\n",
        "# extract_folder = \"butterfly_dataset\"\n",
        "\n",
        "# if not os.path.exists(extract_folder):\n",
        "#     with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(extract_folder)\n",
        "\n",
        "# # ðŸ“Œ Generate Labels from Filenames\n",
        "# image_folder = \"butterfly_dataset/mixed\"  # Path where images are stored\n",
        "\n",
        "# # Get all image filenames\n",
        "# image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp'))]\n",
        "\n",
        "\n",
        "\n",
        "# # Load images and preprocess them for feature extraction\n",
        "# img_size = (224, 224)\n",
        "# images = []\n",
        "# for img_file in image_files:\n",
        "#     img_path = os.path.join(image_folder, img_file)\n",
        "#     img = load_img(img_path, target_size=img_size)\n",
        "#     img_array = img_to_array(img)  # Convert image to array\n",
        "#     img_array = preprocess_input(img_array)  # Apply ResNet preprocessing\n",
        "#     images.append(img_array)\n",
        "\n",
        "# # Convert list of images into a numpy array\n",
        "# images = np.array(images)\n",
        "\n",
        "# # Ensure the shape is correct (num_images, height, width, channels)\n",
        "# print(f\"Image shape: {images.shape}\")\n",
        "\n",
        "# # ðŸ“Œ Use Pre-trained ResNet50 for Feature Extraction\n",
        "# resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# resnet_model.trainable = False  # Freeze the layers\n",
        "\n",
        "# # Extract features from the images\n",
        "# features = resnet_model.predict(np.array(images), batch_size=32)\n",
        "\n",
        "# # Flatten the features for clustering\n",
        "# features_flat = features.reshape(features.shape[0], -1)\n",
        "\n",
        "\n",
        "\n",
        "# # ðŸ“Œ Apply KMeans Clustering to Group Images into Clusters (Pseudo-Labels)\n",
        "# num_clusters = 4  # Set the number of clusters (adjust as needed)\n",
        "# kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "# pseudo_labels = kmeans.fit_predict(features_flat)\n",
        "\n",
        "# # ðŸ“Œ Create a DataFrame with the pseudo-labels\n",
        "# df = pd.DataFrame({'filename': image_files, 'label': pseudo_labels})\n",
        "\n",
        "# # Split into train and validation sets (80% train, 20% validation)\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "\n",
        "# # ðŸ“Œ Image Data Augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rescale=1./255,\n",
        "#     rotation_range=20,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.2,\n",
        "#     horizontal_flip=True\n",
        "# )\n",
        "\n",
        "# # ðŸ“Œ Load Data from DataFrame (Single Folder)\n",
        "# batch_size = 32\n",
        "\n",
        "# train_df[\"label\"] = train_df[\"label\"].astype(str)\n",
        "# val_df[\"label\"] = val_df[\"label\"].astype(str)\n",
        "\n",
        "# train_generator = datagen.flow_from_dataframe(\n",
        "#     train_df,\n",
        "#     directory=image_folder,\n",
        "#     x_col=\"filename\",\n",
        "#     y_col=\"label\",\n",
        "#     target_size=img_size,\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical'\n",
        "# )\n",
        "\n",
        "# val_generator = datagen.flow_from_dataframe(\n",
        "#     val_df,\n",
        "#     directory=image_folder,\n",
        "#     x_col=\"filename\",\n",
        "#     y_col=\"label\",\n",
        "#     target_size=img_size,\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical'\n",
        "# )\n",
        "\n",
        "# # ðŸ“Œ CNN Model Definition\n",
        "# model = Sequential([\n",
        "#     Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D((2,2)),\n",
        "\n",
        "#     Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D((2,2)),\n",
        "\n",
        "#     Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D((2,2)),\n",
        "\n",
        "#     Flatten(),\n",
        "#     Dense(256, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(num_clusters, activation='softmax')  # Number of clusters as output classes\n",
        "# ])\n",
        "\n",
        "# # ðŸ“Œ Compile the Model\n",
        "# model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # ðŸ“Œ Train the Model\n",
        "# history = model.fit(train_generator,\n",
        "#                     validation_data=val_generator,\n",
        "#                     epochs=100, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# # ðŸ“Œ Evaluate the Model\n",
        "# val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
        "# print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# # ðŸ“Œ Plot Training vs Validation Accuracy/Loss\n",
        "# plt.figure(figsize=(12,5))\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.legend()\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.title(\"Training & Validation Accuracy\")\n",
        "\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.plot(history.history['loss'], label='Train Loss')\n",
        "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "# plt.legend()\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Training & Validation Loss\")\n",
        "# plt.show()\n",
        "\n",
        "# # ðŸ“Œ Generate Predictions for Test Data\n",
        "# y_true = val_generator.classes\n",
        "# y_pred = np.argmax(model.predict(val_generator), axis=1)\n",
        "\n",
        "# # ðŸ“Œ Confusion Matrix & Performance Metrics\n",
        "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "# acc = accuracy_score(y_true, y_pred)\n",
        "# f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# print(f\"Test Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# # ðŸ“Œ Plot Confusion Matrix Heatmap\n",
        "# plt.figure(figsize=(6,5))\n",
        "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=train_generator.class_indices.keys(), yticklabels=train_generator.class_indices.keys())\n",
        "# plt.xlabel(\"Predicted Label\")\n",
        "# plt.ylabel(\"True Label\")\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.show()\n",
        "\n",
        "# # ðŸ“Œ Classification Report\n",
        "# print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=train_generator.class_indices.keys()))\n",
        "\n",
        "# # ðŸ“Œ Save Model\n",
        "# model.save(\"butterfly_classifier.h5\")\n"
      ],
      "metadata": {
        "id": "tbiPibzxCz3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import matplotlib.pyplot as plt\n",
        "# import cv2\n",
        "# import os\n",
        "# from tensorflow.keras.preprocessing import image\n",
        "# from google.colab import files  # Use this for Google Colab\n",
        "\n",
        "# # ðŸ“Œ Load the trained model\n",
        "# model = tf.keras.models.load_model(\"butterfly_classifier.h5\")\n",
        "\n",
        "# # ðŸ“Œ Define class names (must match the folder names used during training)\n",
        "# class_names = ['common', 'painted', 'red_based', 'red_spot']  # Modify if needed\n",
        "\n",
        "# # ðŸ“Œ Function to Upload & Predict\n",
        "# def upload_and_predict():\n",
        "#     uploaded = files.upload()  # Opens a file uploader dialog\n",
        "\n",
        "#     for filename in uploaded.keys():\n",
        "#         # Load and preprocess the image\n",
        "#         img_path = filename\n",
        "#         img = image.load_img(img_path, target_size=(224, 224))  # Resize image\n",
        "#         img_array = image.img_to_array(img) / 255.0  # Normalize pixel values\n",
        "#         img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions for batch\n",
        "\n",
        "#         # ðŸ“Œ Make Prediction\n",
        "#         prediction = model.predict(img_array)\n",
        "#         predicted_class = np.argmax(prediction)  # Get the class index\n",
        "#         class_label = class_names[predicted_class]\n",
        "\n",
        "#         # ðŸ“Œ Display Image with Prediction\n",
        "#         plt.imshow(cv2.imread(img_path)[:, :, ::-1])  # Convert BGR to RGB\n",
        "#         plt.title(f\"Predicted: {class_label}\")\n",
        "#         plt.axis(\"off\")\n",
        "#         plt.show()\n",
        "\n",
        "#         print(f\"ðŸ”¹ Model Prediction: {class_label} (Confidence: {max(prediction[0]) * 100:.2f}%)\")\n",
        "\n",
        "# # ðŸ“Œ Run the function\n",
        "# upload_and_predict()"
      ],
      "metadata": {
        "id": "2DhCpU-ADJJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensamble Training"
      ],
      "metadata": {
        "id": "TBX5jbyk3cIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.[Resnet50+Inception+EfficientNet]"
      ],
      "metadata": {
        "id": "zC7ZIZD83b8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from tensorflow.keras.applications import ResNet50, InceptionV3, EfficientNetB0\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
        "# from tensorflow.keras.optimizers import RMSprop\n",
        "# from sklearn.metrics import accuracy_score, roc_curve, auc, precision_recall_curve, confusion_matrix, f1_score\n",
        "\n",
        "# # Define a function to create a model\n",
        "# def create_model(base_model_class, input_shape=(224, 224, 3), num_classes=5):\n",
        "#     base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "#     x = base_model.output\n",
        "#     x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = MaxPooling2D((2, 2))(x)\n",
        "#     x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = MaxPooling2D((2, 2))(x)\n",
        "#     x = GlobalAveragePooling2D()(x)\n",
        "#     x = Dense(256, activation='relu')(x)\n",
        "#     output = Dense(5, activation='softmax')(x)\n",
        "#     model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "#     for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "\n",
        "#     model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Create models\n",
        "# resnet_model = create_model(ResNet50)\n",
        "# inception_model = create_model(InceptionV3)\n",
        "# efficientnet_model = create_model(EfficientNetB0)\n",
        "\n",
        "# # Train the models\n",
        "# history_resnet = resnet_model.fit(X_train_scaled, Y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "# history_inception = inception_model.fit(X_train_scaled, Y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "# history_efficientnet = efficientnet_model.fit(X_train_scaled, Y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "\n",
        "# # Get predictions from all models\n",
        "# resnet_preds_prob = resnet_model.predict(X_test_scaled)\n",
        "# inception_preds_prob = inception_model.predict(X_test_scaled)\n",
        "# efficientnet_preds_prob = efficientnet_model.predict(X_test_scaled)\n",
        "\n",
        "# resnet_preds = np.argmax(resnet_preds_prob, axis=1)\n",
        "# inception_preds = np.argmax(inception_preds_prob, axis=1)\n",
        "# efficientnet_preds = np.argmax(efficientnet_preds_prob, axis=1)\n",
        "\n",
        "# # Majority voting\n",
        "# final_preds = np.array([np.bincount([r, i, e]).argmax() for r, i, e in zip(resnet_preds, inception_preds, efficientnet_preds)])\n",
        "\n",
        "# # Retrain ResNet50 with new labels\n",
        "# resnet_model_final = create_model(ResNet50)\n",
        "# history_final = resnet_model_final.fit(X_test_scaled, final_preds, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
        "\n",
        "\n",
        "# # Generate predictions for visualization\n",
        "# Y_test_pred_prob = resnet_model_final.predict(X_test_scaled)\n",
        "# Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "# Y_test_one_hot = tf.keras.utils.to_categorical(final_preds, num_classes=5)\n",
        "\n",
        "# # Visualizations\n",
        "# def plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history):\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     for i in range(5):\n",
        "#         y_true_binary = (np.argmax(Y_test_one_hot, axis=1) == i).astype(int)\n",
        "#         y_score = Y_test_pred_prob[:, i]\n",
        "#         fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "#         roc_auc = auc(fpr, tpr)\n",
        "#         plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "#     plt.plot([0, 1], [0, 1], 'k--')\n",
        "#     plt.title('ROC Curve')\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.legend(loc='lower right')\n",
        "#     plt.grid()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Confusion Matrix\n",
        "#     conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "#     plt.figure(figsize=(10, 8))\n",
        "#     sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "#     plt.title('Confusion Matrix')\n",
        "#     plt.xlabel('Predicted')\n",
        "#     plt.ylabel('Actual')\n",
        "#     plt.show()\n",
        "\n",
        "#     # Precision-Recall Curve\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     for i in range(5):\n",
        "#         precision, recall, _ = precision_recall_curve(Y_test_one_hot[:, i], Y_test_pred_prob[:, i])\n",
        "#         plt.plot(recall, precision, label=f'Class {i}')\n",
        "#     plt.title('Precision-Recall Curve')\n",
        "#     plt.xlabel('Recall')\n",
        "#     plt.ylabel('Precision')\n",
        "#     plt.legend()\n",
        "#     plt.grid()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Training Curves\n",
        "#     plt.figure(figsize=(12, 4))\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(history.history['accuracy'], label='Training')\n",
        "#     plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "#     plt.title('Model Accuracy')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy')\n",
        "#     plt.legend()\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(history.history['loss'], label='Training')\n",
        "#     plt.plot(history.history['val_loss'], label='Validation')\n",
        "#     plt.title('Model Loss')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Generate all visualizations\n",
        "# plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history_final)\n",
        "\n",
        "# # Print final metrics\n",
        "# print(\"\\nFinal Model Performance:\")\n",
        "# print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "# print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Save the final model\n",
        "# resnet_model_final.save('final_resnet50_model.h5')\n",
        "# print(\"Final ResNet50 model saved.\")"
      ],
      "metadata": {
        "id": "gXMwS7SPhl68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.[Resnet+DenseNet+MobileNet-->Resnet]"
      ],
      "metadata": {
        "id": "ArPbm-h4zBQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from tensorflow.keras.applications import ResNet50, VGG19, DenseNet121\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
        "# from tensorflow.keras.optimizers import RMSprop\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "# from sklearn.metrics import accuracy_score, roc_curve, auc, precision_recall_curve, confusion_matrix, f1_score, r2_score\n",
        "\n",
        "# # Define class labels\n",
        "# class_labels = ['Ilish', 'Chandana', 'Sardin', 'Sardinella', 'Punctatus']\n",
        "\n",
        "# # Define callbacks\n",
        "# checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "# earlystop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "# csv_logger = CSVLogger('training_log.csv')\n",
        "# callbacks = [checkpoint, earlystop, reduce_lr, csv_logger]\n",
        "\n",
        "# # Define a function to create a model\n",
        "# def create_model(base_model_class, input_shape=(224, 224, 3), num_classes=5):\n",
        "#     base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "#     x = base_model.output\n",
        "#     x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = MaxPooling2D((2, 2))(x)\n",
        "#     x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "#     x = MaxPooling2D((2, 2))(x)\n",
        "#     x = GlobalAveragePooling2D()(x)\n",
        "#     x = Dense(256, activation='relu')(x)\n",
        "#     output = Dense(5, activation='softmax')(x)\n",
        "#     model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "#     for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "\n",
        "#     model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Create models\n",
        "# resnet_model = create_model(ResNet50)\n",
        "# densenet_model = create_model(DenseNet121)\n",
        "# vgg19_model = create_model(VGG19)\n",
        "\n",
        "# # Train the models\n",
        "# print(\"Training ResNet50...\")\n",
        "# history_resnet = resnet_model.fit(X_train_scaled, Y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
        "# print(\"Training DenseNet121...\")\n",
        "# history_densenet = densenet_model.fit(X_train_scaled, Y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
        "# print(\"Training VGG19...\")\n",
        "# history_vgg19 = vgg19_model.fit(X_train_scaled, Y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "# # Get predictions from all models\n",
        "# resnet_preds_prob = resnet_model.predict(X_test_scaled)\n",
        "# densenet_preds_prob = densenet_model.predict(X_test_scaled)\n",
        "# vgg19_preds_prob = vgg19_model.predict(X_test_scaled)\n",
        "\n",
        "# resnet_preds = np.argmax(resnet_preds_prob, axis=1)\n",
        "# densenet_preds = np.argmax(densenet_preds_prob, axis=1)\n",
        "# vgg19_preds = np.argmax(vgg19_preds_prob, axis=1)\n",
        "\n",
        "# # Majority voting\n",
        "# final_preds = np.array([np.bincount([r, d, v]).argmax() for r, d, v in zip(resnet_preds, densenet_preds, vgg19_preds)])\n",
        "\n",
        "# # Retrain ResNet50 with new labels\n",
        "# print(\"Retraining ResNet50 with updated labels...\")\n",
        "# resnet_model_final = create_model(ResNet50)\n",
        "# history_final = resnet_model_final.fit(X_test_scaled, final_preds, epochs=20, batch_size=32, verbose=1, validation_split=0.2, callbacks=callbacks)\n",
        "\n",
        "# # Save the final model\n",
        "# resnet_model_final.save('final_resnet50_model.h5')\n",
        "\n",
        "# # Generate predictions for visualization\n",
        "# Y_test_pred_prob = resnet_model_final.predict(X_test_scaled)\n",
        "# Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "# Y_test_one_hot = tf.keras.utils.to_categorical(final_preds, num_classes=5)\n",
        "\n",
        "# # Visualizations\n",
        "# def plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history):\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     for i in range(5):\n",
        "#         y_true_binary = (np.argmax(Y_test_one_hot, axis=1) == i).astype(int)\n",
        "#         y_score = Y_test_pred_prob[:, i]\n",
        "#         fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "#         roc_auc = auc(fpr, tpr)\n",
        "#         plt.plot(fpr, tpr, label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')\n",
        "#     plt.plot([0, 1], [0, 1], 'k--')\n",
        "#     plt.title('ROC Curve')\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.legend(loc='lower right')\n",
        "#     plt.grid()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Confusion Matrix\n",
        "#     conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "#     plt.figure(figsize=(10, 8))\n",
        "#     sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "#     plt.title('Confusion Matrix')\n",
        "#     plt.xlabel('Predicted')\n",
        "#     plt.ylabel('Actual')\n",
        "#     plt.show()\n",
        "\n",
        "#     # R2 Score\n",
        "#     print(f\"R2 Score: {r2_score(Y_test, Y_test_pred):.4f}\")\n",
        "\n",
        "# # Generate all visualizations\n",
        "# plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history_final)\n",
        "\n",
        "# # Print final metrics\n",
        "# print(\"\\nFinal Model Performance:\")\n",
        "# print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "# print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# print(\"Final ResNet50 model saved.\")\n"
      ],
      "metadata": {
        "id": "d0YaBaqOzAfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Single Transfer Model Trianing"
      ],
      "metadata": {
        "id": "_Gqi4q2r3KrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNet50"
      ],
      "metadata": {
        "id": "Xd7lGcEo2sAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jul18GhIy8YC"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "# from tensorflow.keras.optimizers import RMSprop\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, f1_score\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Initialize the ResNet model with transfer learning\n",
        "# input_shape = (224, 224, 3)\n",
        "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# # Add custom CNN layers\n",
        "# x = base_model.output\n",
        "# x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "# x = MaxPooling2D((2, 2))(x)\n",
        "# x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "# x = MaxPooling2D((2, 2))(x)\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dense(256, activation='relu')(x)\n",
        "# output = Dense(5, activation='softmax')(x)\n",
        "\n",
        "# model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# # Freeze base model layers\n",
        "# for layer in base_model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# # Compile model\n",
        "# model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Callbacks\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "# model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# # Train model with callbacks\n",
        "# history = model.fit(X_train_scaled, Y_train,\n",
        "#                     validation_split=0.2,\n",
        "#                     epochs=20, batch_size=32, verbose=1,\n",
        "#                     callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# # Evaluate model\n",
        "# val_loss, val_accuracy = model.evaluate(X_test_scaled, Y_test, verbose=0)\n",
        "# print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# # One-hot encode the test labels (if not already done)\n",
        "# Y_test_one_hot = to_categorical(Y_test, num_classes=5)\n",
        "\n",
        "# # Generate predictions\n",
        "# Y_test_pred_prob = model.predict(X_test_scaled)\n",
        "# Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "\n",
        "# # Define class labels (adjust as per your classes)\n",
        "# class_labels = ['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']\n",
        "\n",
        "# # Visualizations\n",
        "# def plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, class_labels):\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     for i in range(len(class_labels)):\n",
        "#         y_true_binary = (np.argmax(Y_test_one_hot, axis=1) == i).astype(int)\n",
        "#         y_score = Y_test_pred_prob[:, i]\n",
        "#         fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "#         roc_auc = auc(fpr, tpr)\n",
        "#         plt.plot(fpr, tpr, label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')\n",
        "#     plt.plot([0, 1], [0, 1], 'k--')\n",
        "#     plt.title('ROC Curve')\n",
        "#     plt.xlabel('False Positive Rate')\n",
        "#     plt.ylabel('True Positive Rate')\n",
        "#     plt.legend(loc='lower right')\n",
        "#     plt.grid()\n",
        "#     plt.show()\n",
        "\n",
        "#     # Confusion Matrix\n",
        "#     conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "#     plt.figure(figsize=(10, 8))\n",
        "#     sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "#     plt.title('Confusion Matrix')\n",
        "#     plt.xlabel('Predicted')\n",
        "#     plt.ylabel('Actual')\n",
        "#     plt.show()\n",
        "\n",
        "#     # Final metrics\n",
        "#     print(\"\\nFinal Model Performance:\")\n",
        "#     print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "#     print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n",
        "\n",
        "# # Generate all visualizations\n",
        "# plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, class_labels)\n",
        "\n",
        "# # Save the model after training (if not already saved by checkpoint)\n",
        "# model.save('final_resnet_model.h5')\n",
        "# print(\"Final ResNet50 model saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EfficientNetB0"
      ],
      "metadata": {
        "id": "0Lc2h-2mLVjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize EfficientNetB0 model with transfer learning\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "\n",
        "\n",
        "# Changes & Fixes:\n",
        "# âœ… Learning Rate: Increased from 1e-5 â†’ 1e-4\n",
        "# âœ… Batch Normalization: Added after dense layers to improve training stability\n",
        "# âœ… Activation: Replaced ReLU with LeakyReLU to avoid dead neurons\n",
        "# âœ… Fine-Tuning: Unfroze last 20 layers of EfficientNetB0 for better feature learning\n",
        "# âœ… Dropout: Adjusted to 0.5 for stronger regularization\n",
        "# âœ… ReduceLROnPlateau Factor: Changed from 0.5 â†’ 0.7 to reduce LR more effectively\n",
        "# âœ… Epochs: Increased to 30 with early stopping at 7 patience\n",
        "# This should fix the NaN loss issue, improve training stability, and give better performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.layers.LeakyReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "output = Dense(5, activation='softmax')(x)  # 5 classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Unfreeze the last few layers for fine-tuning\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),  # Increased LR\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=4, min_lr=1e-6, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    validation_data=(X_test, Y_test),\n",
        "                    epochs=10, batch_size=32, verbose=1,\n",
        "                    callbacks=[early_stop, lr_reduce, model_checkpoint])\n",
        "\n",
        "# Evaluate Model\n",
        "val_loss, val_accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Training History Plot\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)\n",
        "\n",
        "# Predictions\n",
        "Y_test_pred_prob = model.predict(X_test)\n",
        "Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "\n",
        "# Confusion Matrix & ROC Curve\n",
        "def plot_all_visualizations(Y_test, Y_test_pred, Y_test_pred_prob, class_labels):\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(len(class_labels)):\n",
        "        y_true_binary = (Y_test == i).astype(int)\n",
        "        y_score = Y_test_pred_prob[:, i]\n",
        "        fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# Define class labels\n",
        "class_labels = ['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']\n",
        "\n",
        "# Generate Visualizations\n",
        "plot_all_visualizations(Y_test, Y_test_pred, Y_test_pred_prob, class_labels)\n",
        "\n",
        "# Final Performance Metrics\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "id": "1WJyDwwLLYoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#InceptionV3"
      ],
      "metadata": {
        "id": "iAeVR_i5N7_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the InceptionV3 model with transfer learning\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Add custom CNN layers\n",
        "x = base_model.output\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile model with a lower learning rate to avoid NaN values\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # Reduced learning rate\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks: EarlyStopping and ReduceLROnPlateau\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Normalize data if not already done\n",
        "X_train_scaled = X_train / 255.0\n",
        "X_test_scaled = X_test / 255.0\n",
        "\n",
        "# Train model with callbacks\n",
        "history = model.fit(X_train_scaled, Y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=20, batch_size=32, verbose=1,\n",
        "                    callbacks=[early_stop, lr_reduction])\n",
        "\n",
        "# Evaluate model\n",
        "val_loss, val_accuracy = model.evaluate(X_test_scaled, Y_test, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "def plot_training_history(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Visualizations\n",
        "\n",
        "def plot_all_visualizations(Y_test, Y_test_pred, Y_test_pred_prob, class_labels):\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve for each class\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        y_true_binary = (Y_test == i).astype(int)  # Fix: Directly compare the integer labels\n",
        "        y_score = Y_test_pred_prob[:, i]\n",
        "        fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Make predictions\n",
        "Y_test_pred_prob = model.predict(X_test_scaled)\n",
        "Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)  # Get predicted class labels\n",
        "\n",
        "# Define class labels\n",
        "class_labels = ['Class 1', 'Class 2', 'Class 3', 'Class 4']  # Change based on your classes\n",
        "\n",
        "# Generate visualizations\n",
        "plot_all_visualizations(Y_test, Y_test_pred, Y_test_pred_prob, class_labels)\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "id": "3w5VvADKN67d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions for visualization\n",
        "Y_test_pred_prob = model.predict(X_test_scaled)\n",
        "Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "\n",
        "# Convert labels for visualization\n",
        "Y_test_one_hot = tf.keras.utils.to_categorical(Y_test, num_classes=4)\n",
        "\n",
        "# Visualizations\n",
        "def plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history):\n",
        "    # ROC Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        y_true_binary = (np.argmax(Y_test_one_hot, axis=1) == i).astype(int)\n",
        "        y_score = Y_test_pred_prob[:, i]\n",
        "        fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        precision, recall, _ = precision_recall_curve(Y_test_one_hot[:, i], Y_test_pred_prob[:, i])\n",
        "        plt.plot(recall, precision, label=f'Class {i}')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Training Curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate all visualizations\n",
        "plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history)\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "id": "BeVJAz4R6tsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MobileNetV2"
      ],
      "metadata": {
        "id": "bOBzx7GS4Z92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, accuracy_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the MobileNetV2 model with transfer learning\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Add custom CNN layers\n",
        "x = base_model.output\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train_scaled, Y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate model\n",
        "val_loss, val_accuracy = model.evaluate(X_test_scaled, Y_test, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "sscPvxup4ZoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions for visualization\n",
        "Y_test_pred_prob = model.predict(X_test_scaled)\n",
        "Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "\n",
        "# Convert labels for visualization\n",
        "Y_test_one_hot = tf.keras.utils.to_categorical(Y_test, num_classes=4)\n",
        "\n",
        "# Visualizations\n",
        "def plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history):\n",
        "    # ROC Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        y_true_binary = (np.argmax(Y_test_one_hot, axis=1) == i).astype(int)\n",
        "        y_score = Y_test_pred_prob[:, i]\n",
        "        fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        precision, recall, _ = precision_recall_curve(Y_test_one_hot[:, i], Y_test_pred_prob[:, i])\n",
        "        plt.plot(recall, precision, label=f'Class {i}')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Training Curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate all visualizations\n",
        "plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history)\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "id": "5Fwnegy_6u0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGG-19"
      ],
      "metadata": {
        "id": "GiJS2Cy04guP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, accuracy_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the VGG16 model with transfer learning\n",
        "input_shape = (224, 224, 3)\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Add custom CNN layers\n",
        "x = base_model.output\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "output = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train_scaled, Y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate model\n",
        "val_loss, val_accuracy = model.evaluate(X_test_scaled, Y_test, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Yrsxn9Tk4gX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions for visualization\n",
        "Y_test_pred_prob = model.predict(X_test_scaled)\n",
        "Y_test_pred = np.argmax(Y_test_pred_prob, axis=1)\n",
        "\n",
        "# Convert labels for visualization\n",
        "Y_test_one_hot = tf.keras.utils.to_categorical(Y_test, num_classes=4)\n",
        "\n",
        "# Visualizations\n",
        "def plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history):\n",
        "    # ROC Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        y_true_binary = (np.argmax(Y_test_one_hot, axis=1) == i).astype(int)\n",
        "        y_score = Y_test_pred_prob[:, i]\n",
        "        fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_test_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(4):\n",
        "        precision, recall, _ = precision_recall_curve(Y_test_one_hot[:, i], Y_test_pred_prob[:, i])\n",
        "        plt.plot(recall, precision, label=f'Class {i}')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Training Curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate all visualizations\n",
        "plot_all_visualizations(Y_test, Y_test_one_hot, Y_test_pred, Y_test_pred_prob, history)\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "print(f\"Test Accuracy: {accuracy_score(Y_test, Y_test_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(Y_test, Y_test_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "id": "4PlTopkkLbSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unaPvkKLy8YD"
      },
      "outputs": [],
      "source": [
        "# model.save('efficientnetb0.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ledYtVy8YD"
      },
      "source": [
        "#End"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}